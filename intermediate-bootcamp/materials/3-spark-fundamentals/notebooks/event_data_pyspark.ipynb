{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a085abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "# expr allow to use sql in pyspark code\n",
    "# col to tell spark this is column and look for its name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e53af9",
   "metadata": {},
   "source": [
    "# data frame level \n",
    "-immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cca085-dba2-42eb-a13b-fa64b6e86583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/19 14:23:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+--------------------+----------+--------------------+-------------------+--------------+---------+-----------+\n",
      "| device_id|    user_id|referrer|                host|       url|          event_time|         event_date|browser_family|os_family|device_type|\n",
      "+----------+-----------+--------+--------------------+----------+--------------------+-------------------+--------------+---------+-----------+\n",
      "| 532630305| 1037710827|    NULL| www.zachwilson.tech|         /|2021-03-08 17:27:...|2021-03-08 00:00:00|         Other|    Other|      Other|\n",
      "| 532630305|  925588856|    NULL|    www.eczachly.com|         /|2021-05-10 11:26:...|2021-05-10 00:00:00|         Other|    Other|      Other|\n",
      "| 532630305|-1180485268|    NULL|admin.zachwilson....|         /|2021-02-17 16:19:...|2021-02-17 00:00:00|         Other|    Other|      Other|\n",
      "| 532630305|-1044833855|    NULL| www.zachwilson.tech|         /|2021-09-24 15:53:...|2021-09-24 00:00:00|         Other|    Other|      Other|\n",
      "| 532630305|  747494706|    NULL| www.zachwilson.tech|         /|2021-09-26 16:03:...|2021-09-26 00:00:00|         Other|    Other|      Other|\n",
      "| 532630305|  747494706|    NULL|admin.zachwilson....|         /|2021-02-21 16:08:...|2021-02-21 00:00:00|         Other|    Other|      Other|\n",
      "| 532630305| -824540328|    NULL|admin.zachwilson....|         /|2021-09-28 17:23:...|2021-09-28 00:00:00|         Other|    Other|      Other|\n",
      "| 532630305| -824540328|    NULL|    www.eczachly.com|         /|2021-09-29 01:22:...|2021-09-29 00:00:00|         Other|    Other|      Other|\n",
      "| 532630305| 1833036683|    NULL|admin.zachwilson....|         /|2021-01-24 03:15:...|2021-01-24 00:00:00|         Other|    Other|      Other|\n",
      "| 532630305|-2134824313|    NULL|    www.eczachly.com|         /|2021-01-25 00:03:...|2021-01-25 00:00:00|         Other|    Other|      Other|\n",
      "|-906264142|-1809929467|    NULL|admin.zachwilson....|/.git/HEAD|2021-02-22 01:36:...|2021-02-22 00:00:00|          curl|    Other|      Other|\n",
      "|-906264142| 2002285749|    NULL|    www.eczachly.com|         /|2021-02-22 02:25:...|2021-02-22 00:00:00|          curl|    Other|      Other|\n",
      "| 532630305|-1562965412|    NULL| www.zachwilson.tech|         /|2021-01-30 20:46:...|2021-01-30 00:00:00|         Other|    Other|      Other|\n",
      "| 532630305|-1099860451|    NULL|    www.eczachly.com|         /|2021-02-04 23:49:...|2021-02-04 00:00:00|         Other|    Other|      Other|\n",
      "|-906264142| 1246896869|    NULL| www.zachwilson.tech|         /|2021-02-22 02:50:...|2021-02-22 00:00:00|          curl|    Other|      Other|\n",
      "|-906264142| -629331502|    NULL|admin.zachwilson....|/.git/HEAD|2021-02-22 23:51:...|2021-02-22 00:00:00|          curl|    Other|      Other|\n",
      "|-906264142|-1913422462|    NULL|    www.eczachly.com|         /|2021-02-23 00:17:...|2021-02-23 00:00:00|          curl|    Other|      Other|\n",
      "| 532630305|   50429624|    NULL|    www.eczachly.com|         /|2022-12-28 01:38:...|2022-12-28 00:00:00|         Other|    Other|      Other|\n",
      "| 532630305|  222389292|    NULL| www.zachwilson.tech|         /|2022-12-28 05:23:...|2022-12-28 00:00:00|         Other|    Other|      Other|\n",
      "| 532630305| -779924777|    NULL| www.zachwilson.tech|         /|2022-12-28 16:45:...|2022-12-28 00:00:00|         Other|    Other|      Other|\n",
      "+----------+-----------+--------+--------------------+----------+--------------------+-------------------+--------------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# start Spark session with builder and customized app name\n",
    "spark = SparkSession.builder.appName(\"Jupyter\").getOrCreate()\n",
    "\n",
    "# will show spark session details\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ac7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lazy - read events data and create a new column event_data to use in Iceberg later on\n",
    "# date_trunc to make time to be like 00:00:00:00\n",
    "events = spark.read.option(\"header\", \"true\").csv(\"/home/iceberg/data/events.csv\").withColumn(\"event_date\", expr(\"DATE_TRUNC('day', event_time)\"))\n",
    "# lazy - read devices data\n",
    "devices = spark.read.option(\"header\",\"true\").csv(\"/home/iceberg/data/devices.csv\")\n",
    "\n",
    "# lazy - left join two dataframes\n",
    "df = events.join(devices,on=\"device_id\",how=\"left\")\n",
    "# lazy - rename columns\n",
    "df = df.withColumnsRenamed({'browser_type': 'browser_family', 'os_type': 'os_family'})\n",
    "\n",
    "# action - show() to show some rows, similar to take(n)\n",
    "# must avoid using collect() to show all rows because it will cause OOM issue and kill kernel \n",
    "## only use collect() when it's small table like aggregated table\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62178ec5",
   "metadata": {},
   "source": [
    ".sortWithinPartitions() sorts within partitions, whereas .sort() is a global sort, which is very slow (global sort = True)     \n",
    "\n",
    "Note - exchange is synonymous with Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce068df-3e21-429a-8716-abdd13e9406c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------+---------+------------------+\n",
      "|  device_id|    user_id|            referrer|                host|                 url|          event_time|         event_date|browser_family|os_family|       device_type|\n",
      "+-----------+-----------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------+---------+------------------+\n",
      "|  532630305| 1129583063|                NULL|admin.zachwilson....|                   /|2021-01-07 09:21:...|2021-01-07 00:00:00|         Other|    Other|             Other|\n",
      "| 1088283544| -648945006|                NULL|    www.eczachly.com|                   /|2021-01-07 02:58:...|2021-01-07 00:00:00|      PetalBot|  Android|Generic Smartphone|\n",
      "| -158310583|-1871780024|                NULL|    www.eczachly.com|                   /|2021-01-07 04:17:...|2021-01-07 00:00:00|      PetalBot|    Other|            Spider|\n",
      "| 1088283544|  203689086|                NULL|    www.eczachly.com|/blog/what-exactl...|2021-01-07 10:03:...|2021-01-07 00:00:00|      PetalBot|  Android|Generic Smartphone|\n",
      "|  532630305|-1180485268|                NULL|    www.eczachly.com|                   /|2021-01-07 18:45:...|2021-01-07 00:00:00|         Other|    Other|             Other|\n",
      "|  532630305| 1129583063|                NULL|    www.eczachly.com|                   /|2021-01-07 21:57:...|2021-01-07 00:00:00|         Other|    Other|             Other|\n",
      "| -158310583|-1381834161|                NULL|    www.eczachly.com|                   /|2021-01-07 23:07:...|2021-01-07 00:00:00|      PetalBot|    Other|            Spider|\n",
      "|  532630305|-1373330946|                NULL| www.zachwilson.tech|  /api/v1/spark-post|2021-01-07 18:53:...|2021-01-07 00:00:00|         Other|    Other|             Other|\n",
      "|  532630305|-1180485268|                NULL| www.zachwilson.tech|                   /|2021-01-07 19:20:...|2021-01-07 00:00:00|         Other|    Other|             Other|\n",
      "| 1957784035|-1617088793|https://www.zachw...| www.zachwilson.tech|            /contact|2021-01-07 00:19:...|2021-01-07 00:00:00|        Chrome|  Windows|             Other|\n",
      "| 1957784035|-1617088793|https://www.zachw...| www.zachwilson.tech|     /api/v1/contact|2021-01-07 00:19:...|2021-01-07 00:00:00|        Chrome|  Windows|             Other|\n",
      "| 1957784035|-1617088793|https://www.zachw...| www.zachwilson.tech|            /contact|2021-01-07 00:19:...|2021-01-07 00:00:00|        Chrome|  Windows|             Other|\n",
      "| 1141939293| -267976675|https://www.zachw...| www.zachwilson.tech|            /contact|2021-01-07 01:57:...|2021-01-07 00:00:00|        Chrome|  Windows|             Other|\n",
      "| 1141939293| -267976675|https://www.zachw...| www.zachwilson.tech|     /api/v1/contact|2021-01-07 01:57:...|2021-01-07 00:00:00|        Chrome|  Windows|             Other|\n",
      "| 1141939293| -267976675|https://www.zachw...| www.zachwilson.tech|            /contact|2021-01-07 01:57:...|2021-01-07 00:00:00|        Chrome|  Windows|             Other|\n",
      "| 1800580819|  604069040|                NULL| www.zachwilson.tech|                   /|2021-01-07 02:24:...|2021-01-07 00:00:00| Chrome Mobile|  Android|Generic Smartphone|\n",
      "| 2066336516|-2135898320|http://www.laurel...| www.zachwilson.tech|                   /|2021-01-07 04:57:...|2021-01-07 00:00:00|       Firefox|  Windows|             Other|\n",
      "|-1217993711|  -43429336|                NULL| www.zachwilson.tech|            /contact|2021-01-07 04:57:...|2021-01-07 00:00:00|     AhrefsBot|    Other|            Spider|\n",
      "|-1307168836| 1198758947|                NULL| www.zachwilson.tech|               /blog|2021-01-07 09:57:...|2021-01-07 00:00:00|        Chrome|  Windows|             Other|\n",
      "| 1141939293|-1894709267|https://www.zachw...| www.zachwilson.tech|                   /|2021-01-07 10:12:...|2021-01-07 00:00:00|        Chrome|  Windows|             Other|\n",
      "+-----------+-----------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+-----------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------+---------+------------------+\n",
      "|  device_id|    user_id|            referrer|                host|                 url|          event_time|         event_date|browser_family|os_family|       device_type|\n",
      "+-----------+-----------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------+---------+------------------+\n",
      "| -643696601| 1272828233|                NULL|admin.zachwilson....|                   /|2021-01-02 13:53:...|2021-01-02 00:00:00|        Chrome|  Windows|             Other|\n",
      "|  532630305|  747494706|                NULL|admin.zachwilson....|                   /|2021-01-02 19:36:...|2021-01-02 00:00:00|         Other|    Other|             Other|\n",
      "|  898871897| 2110046626|                NULL|admin.zachwilson....|       /wp-login.php|2021-01-02 19:57:...|2021-01-02 00:00:00|        Chrome|    Linux|             Other|\n",
      "| -643696601| 1272828233|                NULL|admin.zachwilson....|                   /|2021-01-02 21:05:...|2021-01-02 00:00:00|        Chrome|  Windows|             Other|\n",
      "| -643696601| 1272828233|                NULL|admin.zachwilson....|                   /|2021-01-02 21:37:...|2021-01-02 00:00:00|        Chrome|  Windows|             Other|\n",
      "|-2012543895| 1399665425|                NULL|    www.eczachly.com|                   /|2021-01-02 00:20:...|2021-01-02 00:00:00|     Googlebot|    Other|            Spider|\n",
      "| -290659081|  125243313|                NULL|    www.eczachly.com|                   /|2021-01-02 02:06:...|2021-01-02 00:00:00|       bingbot|    Other|            Spider|\n",
      "| -290659081|  632739597|                NULL|    www.eczachly.com|/blog/what-exactl...|2021-01-02 02:58:...|2021-01-02 00:00:00|       bingbot|    Other|            Spider|\n",
      "| -290659081|-1780827820|                NULL|    www.eczachly.com|                   /|2021-01-02 04:45:...|2021-01-02 00:00:00|       bingbot|    Other|            Spider|\n",
      "| -290659081|  632739597|                NULL|    www.eczachly.com|                   /|2021-01-02 05:14:...|2021-01-02 00:00:00|       bingbot|    Other|            Spider|\n",
      "| -158310583| 1047962242|                NULL|    www.eczachly.com|                   /|2021-01-02 11:40:...|2021-01-02 00:00:00|      PetalBot|    Other|            Spider|\n",
      "| -290659081|  273700037|                NULL|    www.eczachly.com|                   /|2021-01-02 07:51:...|2021-01-02 00:00:00|       bingbot|    Other|            Spider|\n",
      "| -643696601| 1272828233|                NULL|    www.eczachly.com|                   /|2021-01-02 08:14:...|2021-01-02 00:00:00|        Chrome|  Windows|             Other|\n",
      "| 1088283544|  210988258|                NULL|    www.eczachly.com|            /contact|2021-01-02 11:11:...|2021-01-02 00:00:00|      PetalBot|  Android|Generic Smartphone|\n",
      "| -290659081|  273700037|                NULL|    www.eczachly.com|                   /|2021-01-02 11:23:...|2021-01-02 00:00:00|       bingbot|    Other|            Spider|\n",
      "| -290659081|  632739597|                NULL|    www.eczachly.com|        /sitemap.xml|2021-01-02 14:10:...|2021-01-02 00:00:00|       bingbot|    Other|            Spider|\n",
      "| -290659081|  659201289|                NULL|    www.eczachly.com|/blog/life-of-a-s...|2021-01-02 15:53:...|2021-01-02 00:00:00|       bingbot|    Other|            Spider|\n",
      "| -784483831| 1072106763|https://www.zachw...| www.zachwilson.tech|            /contact|2021-01-02 09:37:...|2021-01-02 00:00:00|        Chrome|  Windows|             Other|\n",
      "| -223216734| 1744817842|                NULL| www.zachwilson.tech|/graph/stock-tick...|2021-01-02 00:23:...|2021-01-02 00:00:00|   FacebookBot|    Other|            Spider|\n",
      "| 1957784035| 1445055201|https://www.zachw...| www.zachwilson.tech|            /contact|2021-01-02 10:17:...|2021-01-02 00:00:00|        Chrome|  Windows|             Other|\n",
      "+-----------+-----------+--------------------+--------------------+--------------------+--------------------+-------------------+--------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lazy - repartition to 10 by event_date (local - within own partition) \n",
    "# -> sort data in each partition with event_date and host \n",
    "# -> create a new column event_time by converting event_time to timestamp\n",
    "sorted = df.repartition(10, col(\"event_date\"))\\\n",
    "    .sortWithinPartitions(col(\"event_date\"), col(\"host\"))\\\n",
    "    .withColumn(\"event_time\", col(\"event_time\").cast(\"timestamp\")) \n",
    "\n",
    "# lazy - repartition to 10 by event_date (global) \n",
    "# -> sort all data not only within each partition \n",
    "# -> create a new column event_time by converting event_time to timestamp\n",
    "sortedTwo = df.repartition(10, col(\"event_date\"))\\\n",
    "    .sort(col(\"event_date\"), col(\"host\"))\\\n",
    "    .withColumn(\"event_time\", col(\"event_time\").cast(\"timestamp\")) \n",
    "\n",
    "# action\n",
    "sorted.show()\n",
    "sortedTwo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7517ba-ebfa-4b94-aa1a-10f9717174fe",
   "metadata": {},
   "source": [
    "**query plan**\n",
    "\n",
    "- use explain() to show how query plan works in order: it start from left side command\n",
    "- a word \"exchange\" in query plan means shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0bb63-2ef0-4a53-af25-881fbd785d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [user_id#17, device_id#18, referrer#19, host#20, url#21, cast(event_time#22 as timestamp) AS event_time#288, event_date#29]\n",
      "   +- Sort [event_date#29 ASC NULLS FIRST, host#20 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(event_date#29, 10), REPARTITION_BY_NUM, [plan_id=294]\n",
      "         +- Project [user_id#17, device_id#18, referrer#19, host#20, url#21, event_time#22, date_trunc(day, cast(event_time#22 as timestamp), Some(Etc/UTC)) AS event_date#29]\n",
      "            +- FileScan csv [user_id#17,device_id#18,referrer#19,host#20,url#21,event_time#22] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/events.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<user_id:string,device_id:string,referrer:string,host:string,url:string,event_time:string>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [user_id#17, device_id#18, referrer#19, host#20, url#21, cast(event_time#22 as timestamp) AS event_time#296, event_date#29]\n",
      "   +- Sort [event_date#29 ASC NULLS FIRST, host#20 ASC NULLS FIRST], true, 0\n",
      "      +- Exchange rangepartitioning(event_date#29 ASC NULLS FIRST, host#20 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=316]\n",
      "         +- Exchange hashpartitioning(event_date#29, 10), REPARTITION_BY_NUM, [plan_id=312]\n",
      "            +- Project [user_id#17, device_id#18, referrer#19, host#20, url#21, event_time#22, date_trunc(day, cast(event_time#22 as timestamp), Some(Etc/UTC)) AS event_date#29]\n",
      "               +- FileScan csv [user_id#17,device_id#18,referrer#19,host#20,url#21,event_time#22] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/iceberg/data/events.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<user_id:string,device_id:string,referrer:string,host:string,url:string,event_time:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted = df.repartition(10, col(\"event_date\"))\\\n",
    "    .sortWithinPartitions(col(\"event_date\"), col(\"host\"))\\\n",
    "    .withColumn(\"event_time\", col(\"event_time\").cast(\"timestamp\")) \n",
    "\n",
    "sortedTwo = df.repartition(10, col(\"event_date\"))\\\n",
    "    .sort(col(\"event_date\"), col(\"host\"))\\\n",
    "    .withColumn(\"event_time\", col(\"event_time\").cast(\"timestamp\")) \n",
    "\n",
    "# action\n",
    "sorted.explain()\n",
    "sortedTwo.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a187e4",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4519cb5",
   "metadata": {},
   "source": [
    "create bootcamp database for Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d800dca7-2737-4192-b5c0-c1806c105e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE DATABASE IF NOT EXISTS bootcamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba72511",
   "metadata": {},
   "source": [
    "drop table events if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e83cd813-d5c0-4d67-8285-849b882b8bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS bootcamp.events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e814b44-f2c9-4e8f-b9f9-d5550f65331b",
   "metadata": {},
   "source": [
    "create Iceberg table named events\n",
    "\n",
    "partitioned by whatever columns (using date_trunc? for years or hours or days) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1b197a9-1b63-4130-acbe-01418eede0e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.events (\n",
    "    url STRING,\n",
    "    referrer STRING,\n",
    "    browser_family STRING,\n",
    "    os_family STRING,\n",
    "    device_family STRING,\n",
    "    host STRING,\n",
    "    event_time TIMESTAMP,\n",
    "    event_date DATE\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (event_date);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b403139c-f342-426a-a6de-81cd86d1cd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS bootcamp.events_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c40b143f-295e-4875-bd7f-12409312b800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.events_sorted (\n",
    "    url STRING,\n",
    "    referrer STRING,\n",
    "    browser_family STRING,\n",
    "    os_family STRING,\n",
    "    device_family STRING,\n",
    "    host STRING,\n",
    "    event_time TIMESTAMP,\n",
    "    event_date DATE\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (event_date);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38f4912a-11a2-4326-9313-be2aa53b2a53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE bootcamp.events_unsorted;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00c86e79-a911-464c-ad58-acc92859dcc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.events_unsorted (\n",
    "    url STRING,\n",
    "    referrer STRING,\n",
    "    browser_family STRING,\n",
    "    os_family STRING,\n",
    "    device_family STRING,\n",
    "    host STRING,\n",
    "    event_time TIMESTAMP,\n",
    "    event_date DATE\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (year(event_date));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b22ecad",
   "metadata": {},
   "source": [
    "- sort partitioned files can reduce total files size if it starts reading from low cardinality column first (low unique data) to higher cardinality column (high unique data)\n",
    "- can see which column has less unique data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aed9c78c-5dc7-471c-a6ed-d2b2cab6ff61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>event_date</th>\n",
       "            <th>browser</th>\n",
       "            <th>os</th>\n",
       "            <th>host</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>931</td>\n",
       "            <td>216</td>\n",
       "            <td>31</td>\n",
       "            <td>261</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+------------+---------+----+------+\n",
       "| event_date | browser | os | host |\n",
       "+------------+---------+----+------+\n",
       "|        931 |     216 | 31 |  261 |\n",
       "+------------+---------+----+------+"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/19 18:09:57 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-efdbbb68-05f5-4c52-9940-5d8a336fe60e. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-efdbbb68-05f5-4c52-9940-5d8a336fe60e\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:166)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$addShutdownHook$2(DiskBlockManager.scala:346)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Cannot run program \"rm\": error=0, Failed to exec spawn helper: pid: 4825, signal: 15\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:162)\n",
      "\t... 26 more\n",
      "Caused by: java.io.IOException: error=0, Failed to exec spawn helper: pid: 4825, signal: 15\n",
      "\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n",
      "\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:314)\n",
      "\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:244)\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\n",
      "\t... 28 more\n",
      "25/12/19 18:09:57 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-efdbbb68-05f5-4c52-9940-5d8a336fe60e/2d. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-efdbbb68-05f5-4c52-9940-5d8a336fe60e/2d\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:130)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$addShutdownHook$2(DiskBlockManager.scala:346)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/12/19 18:09:57 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-efdbbb68-05f5-4c52-9940-5d8a336fe60e. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-efdbbb68-05f5-4c52-9940-5d8a336fe60e\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2122)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:550)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/12/19 18:09:57 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-efdbbb68-05f5-4c52-9940-5d8a336fe60e/12. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-efdbbb68-05f5-4c52-9940-5d8a336fe60e/12\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:130)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$addShutdownHook$2(DiskBlockManager.scala:346)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/12/19 18:09:57 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-efdbbb68-05f5-4c52-9940-5d8a336fe60e/12. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-efdbbb68-05f5-4c52-9940-5d8a336fe60e/12\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:130)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:117)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2122)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:550)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "select  count(distinct event_date) as event_date, count(distinct browser_family) as browser, count(distinct os_family) as os, count(distinct host) as host\n",
    "from demo.bootcamp.events_unsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1254bc-9ecf-4c86-bfd9-de81ecfbb78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition into 4 parts by event_date and create new column \"event_time\" with converting itself to be timestamp\n",
    "## shuffle once to repartition but unsorted\n",
    "start_df = df.repartition(4, col(\"event_date\")).withColumn(\"event_time\", col(\"event_time\").cast(\"timestamp\")) \\\n",
    "\n",
    "# sort the unsorted dataframe by using low cardinality (low unique data) first     \n",
    "#first_sort_df = start_df.sortWithinPartitions(col(\"event_date\"), col(\"browser_family\"), col(\"host\"))\n",
    "first_sort_df = start_df.sortWithinPartitions(col(\"os_family\"), col(\"browser_family\"), col(\"event_date\"))\n",
    "\n",
    "\n",
    "# write dataframes into tables\n",
    "start_df.write.mode(\"overwrite\").saveAsTable(\"bootcamp.events_unsorted\")\n",
    "first_sort_df.write.mode(\"overwrite\").saveAsTable(\"bootcamp.events_sorted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a2ab5c-9311-447c-ba69-de2d47b0464a",
   "metadata": {},
   "source": [
    "**Iceberg -order in name**     \n",
    "\n",
    "demo.bootcamp.events_sorted.files -> highest level of data . database . table . meta data of the files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d207a11b-b0a4-4662-bbe9-747d8f67be7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>size</th>\n",
       "            <th>num_files</th>\n",
       "            <th>sorted</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>4992940</td>\n",
       "            <td>4</td>\n",
       "            <td>sorted</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>5556664</td>\n",
       "            <td>4</td>\n",
       "            <td>unsorted</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------+-----------+----------+\n",
       "|    size | num_files |   sorted |\n",
       "+---------+-----------+----------+\n",
       "| 4992940 |         4 |   sorted |\n",
       "| 5556664 |         4 | unsorted |\n",
       "+---------+-----------+----------+"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT SUM(file_size_in_bytes) as size, COUNT(1) as num_files, 'sorted' \n",
    "FROM demo.bootcamp.events_sorted.files\n",
    "\n",
    "UNION ALL\n",
    "SELECT SUM(file_size_in_bytes) as size, COUNT(1) as num_files, 'unsorted' \n",
    "FROM demo.bootcamp.events_unsorted.files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93db4d6-ac15-4d0e-83da-77b93ad618da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>size</th>\n",
       "        <th>num_files</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>3145713</td>\n",
       "        <td>5</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------+-----------+\n",
       "|    size | num_files |\n",
       "+---------+-----------+\n",
       "| 3145713 |         5 |\n",
       "+---------+-----------+"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT SUM(file_size_in_bytes) as size, COUNT(1) as num_files \n",
    "FROM demo.bootcamp.events.files;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a37619",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT COUNT(1) \n",
    "FROM bootcamp.matches_bucketed.files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
