{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9c5c7c-afc7-4f0c-9593-094ee6406df7",
   "metadata": {},
   "source": [
    "Need to run spark start session?: \n",
    "\n",
    "these notebooks share the same Iceberg storage so tables can be call from other notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11897715-0131-4689-9712-8a338a3fcfc7",
   "metadata": {},
   "source": [
    "**Read data from csv files**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d9e43b-12d0-442b-87e9-e8e961ac230d",
   "metadata": {},
   "source": [
    "import org.apache.spark.sql.functions.{broadcast, split, lit}   \n",
    "== In python: from pyspark.sql.functions import broadcast, split, lit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "374411c5-a48f-4739-9031-d638638633a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.{broadcast, split, lit}\n",
       "matchesBucketed: org.apache.spark.sql.DataFrame = [match_id: string, mapid: string ... 8 more fields]\n",
       "matchDetailsBucketed: org.apache.spark.sql.DataFrame = [match_id: string, player_gamertag: string ... 34 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.{broadcast, split, lit}\n",
    "\n",
    "\n",
    "val matchesBucketed = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/matches.csv\")\n",
    "val matchDetailsBucketed =  spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/match_details.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799db8c6-d7a7-41f1-b110-c819a80410fc",
   "metadata": {},
   "source": [
    "**create new table: bootcamp.matches_bucketed in icebreg**\n",
    "\n",
    "partitioned into 16 buckets by match_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc8adb02-d5bd-4e84-a671-48991772d233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bucketedDDL: String =\n",
       "\"\n",
       "CREATE TABLE IF NOT EXISTS bootcamp.matches_bucketed (\n",
       "    match_id STRING,\n",
       "    is_team_game BOOLEAN,\n",
       "    playlist_id STRING,\n",
       "    completion_date TIMESTAMP\n",
       ")\n",
       "USING iceberg\n",
       "PARTITIONED BY (completion_date, bucket(16, match_id));\n",
       "\"\n",
       "res3: org.apache.spark.sql.DataFrame = []\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"DROP TABLE IF EXISTS bootcamp.matches_bucketed\"\"\")\n",
    "val bucketedDDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.matches_bucketed (\n",
    "    match_id STRING,\n",
    "    is_team_game BOOLEAN,\n",
    "    playlist_id STRING,\n",
    "    completion_date TIMESTAMP\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (completion_date, bucket(16, match_id));\n",
    "\"\"\"\n",
    "spark.sql(bucketedDDL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1296a1b8-1eea-4b9d-88b7-523ad801e575",
   "metadata": {},
   "source": [
    "**append data from matchesBucketed to bootcamp.matches_bucketed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1eecb6-ca9a-4b5c-b046-b3a0dd1ff3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "matchesBucketed.select(\n",
    "    $\"match_id\", $\"is_team_game\", $\"playlist_id\", $\"completion_date\"\n",
    "    )\n",
    "    .write.mode(\"append\")\n",
    "    .partitionBy(\"completion_date\")\n",
    "  .bucketBy(16, \"match_id\").saveAsTable(\"bootcamp.matches_bucketed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0058a4e-1254-4973-9606-09ddf4ea95ce",
   "metadata": {},
   "source": [
    "**create bootcamp.match_details_bucketed table in iceberg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c7979-1903-44e3-9e46-5f9c571faf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "val bucketedDetailsDDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.match_details_bucketed (\n",
    "   match_id STRING,\n",
    "   player_gamertag STRING,\n",
    "   player_total_kills INTEGER,\n",
    "   player_total_deaths INTEGER\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (bucket(16, match_id));\n",
    "\"\"\"\n",
    "spark.sql(bucketedDetailsDDL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d6dd18-78f2-46db-93e8-b7b313c8879c",
   "metadata": {},
   "source": [
    "**insert data to bucketedDetailsDDL**\n",
    "\n",
    "but this one not using partition from sql    \n",
    "using bucketBy() instead    \n",
    "     \n",
    "this one will have only 16 files as it has 16 buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15762df2-983a-4aca-99a6-ecc26ebd1523",
   "metadata": {},
   "outputs": [],
   "source": [
    "matchDetailsBucketed.select(\n",
    "    $\"match_id\", $\"player_gamertag\", $\"player_total_kills\", $\"player_total_deaths\")\n",
    "    .write.mode(\"append\")\n",
    "  .bucketBy(16, \"match_id\").saveAsTable(\"bootcamp.match_details_bucketed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f83d09-3bed-4061-a483-7ab065b7a99a",
   "metadata": {},
   "source": [
    "// disable broadcast join to see if it will use bucket join because bucket join requires two large tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472b29b-f66a-4bc8-a604-ec2a3b8b50f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbc66b-f511-4a6b-8ca3-fec396e0c583",
   "metadata": {},
   "outputs": [],
   "source": [
    "// create tempview to use spark sql\n",
    "matchesBucketed.createOrReplaceTempView(\"matches\")\n",
    "matchDetailsBucketed.createOrReplaceTempView(\"match_details\")\n",
    "\n",
    "// explain both query plans\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM bootcamp.match_details_bucketed mdb \n",
    "    JOIN bootcamp.matches_bucketed md \n",
    "        ON mdb.match_id = md.match_id AND md.completion_date = DATE('2016-01-01')\n",
    "\"\"\").explain()\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM match_details mdb \n",
    "    JOIN matches md \n",
    "        ON mdb.match_id = md.match_id\n",
    "\"\"\").explain()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b3ac0-11ef-4e57-8509-bdfbebe3bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "// spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"1000000000000\")\n",
    "\n",
    "// val broadcastFromThreshold = matches.as(\"m\").join(matchDetails.as(\"md\"), $\"m.match_id\" === $\"md.match_id\")\n",
    "//   .select($\"m.completion_date\", $\"md.player_gamertag\",  $\"md.player_total_kills\")\n",
    "//   .take(5)\n",
    "\n",
    "// val explicitBroadcast = matches.as(\"m\").join(broadcast(matchDetails).as(\"md\"), $\"m.match_id\" === $\"md.match_id\")\n",
    "//   .select($\"md.*\", split($\"completion_date\", \" \").getItem(0).as(\"ds\"))\n",
    "\n",
    "// val bucketedValues = matchDetailsBucketed.as(\"mdb\").join(matchesBucketed.as(\"mb\"), $\"mb.match_id\" === $\"mdb.match_id\").explain()\n",
    "// // .take(5)\n",
    "\n",
    "// val values = matchDetailsBucketed.as(\"m\").join(matchesBucketed.as(\"md\"), $\"m.match_id\" === $\"md.match_id\").explain()\n",
    "\n",
    "// explicitBroadcast.write.mode(\"overwrite\").insertInto(\"match_details_bucketed\")\n",
    "\n",
    "// matches.withColumn(\"ds\", split($\"completion_date\", \" \").getItem(0)).write.mode(\"overwrite\").insertInto(\"matches_bucketed\")\n",
    "\n",
    "// spark.sql(bucketedSQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c48d6d-b3cd-466e-8422-979fe2cef687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
